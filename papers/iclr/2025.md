## Learning Theory

[1] Kim, S., Mishkin, A., & Pilanci, M. (2024). Exploring the loss landscape of regularized neural networks via convex duality. arXiv preprint arXiv:2411.07729. [OpenReview](https://openreview.net/forum?id=4xWQS2z77v) (Oral)

[2] Li, H., Zhang, Y., Zhang, S., Wang, M., Liu, S., & Chen, P. Y. When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers. In The Thirteenth International Conference on Learning Representations. [OpenReview](https://openreview.net/forum?id=vRvVVb0NAz) (Oral)

[3] On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent. [OpenReview](https://openreview.net/forum?id=97rOQDPmk2) (Spotlight)

[4] Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization. [OpenReview](https://openreview.net/forum?id=hXm0Wu2U9K) (Spotlight)

[5] Decomposition Polyhedra of Piecewise Linear Functions. [OpenReview](https://openreview.net/forum?id=vVCHWVBsLH) (Spotlight)

[6] Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees. [OpenReview](https://openreview.net/forum?id=7liN6uHAQZ) (Poster)

[7] Bridging the Gap Between f-divergences and Bayes Hilbert Spaces. [OpenReview](https://openreview.net/forum?id=m5qpn0KTMZ) (Poster)

[8] Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD. [OpenReview](https://openreview.net/forum?id=pSdE7PIA64) (Poster)

## Optimization

[1] Bullins, B. (2024). Tight Lower Bounds under Asymmetric High-Order H\" older Smoothness and Uniform Convexity. arXiv preprint arXiv:2409.10773. [OpenReview](https://openreview.net/forum?id=fMTPkDEhLQ) (Oral)

[2] Chen, L., Liu, C., & Zhang, J. (2024). Second-order min-max optimization with lazy hessians. arXiv preprint arXiv:2410.09568. [OpenReview](https://openreview.net/forum?id=ijbA5swmoK) (Oral)

[3] Hollender, A., Maystre, G., & Nagarajan, S. G. (2024). The Complexity of Two-Team Polymatrix Games with Independent Adversaries. arXiv preprint arXiv:2409.07398. [OpenReview](https://openreview.net/forum?id=9VGTk2NYjF) (Oral)

[4] Classic but Everlasting: Traditional Gradient-Based Algorithms Converges Fast Even in Time-Varying Multi-Player Games. [OpenReview](https://openreview.net/forum?id=t8FG4cJuL3) (Oral)

[5] Fagnou, E., Caillon, P., Delattre, B., & Allauzen, A. (2025). Accelerated Training through Iterative Gradient Propagation Along the Residual Path. arXiv preprint arXiv:2501.17086. [OpenReview](https://openreview.net/forum?id=JDm7oIcx4Y) (Oral)

[6] Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation. [OpenReview](https://openreview.net/forum?id=hJ1BaJ5ELp) (Spotlight)

[7] Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization. [OpenReview](https://openreview.net/forum?id=rdAbEn5DZt) (Spotlight)

[8] Antoniadis, A., Eliáš, M., Polak, A., & Venzin, M. (2024). Approximation Algorithms for Combinatorial Optimization with Predictions. arXiv preprint arXiv:2411.16600. [OpenReview](https://openreview.net/forum?id=AEFVa6VMu1) (Spotlight)

[9] Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization. [OpenReview](https://openreview.net/forum?id=tuu4de7HL1) (Spotlight)

[10] Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations. [OpenReview](https://openreview.net/forum?id=ywFOSIT9ik) (Spotlight)

[11] Nesterov acceleration in benignly non-convex landscapes. [OpenReview](https://openreview.net/forum?id=YwJkv2YqBq)  (Spotlight)

*[12] Differentiable Integer Linear Programming. [OpenReview](https://openreview.net/forum?id=FPfCUJTsCn) (Spotlight)

[13] Universal generalization guarantees for Wasserstein distributionally robust models. [OpenReview](https://openreview.net/forum?id=0h6v4SpLCY) (Spotlight)

[14] Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity

[15] Convex Formulations for Training Two-Layer ReLU Neural Networks. [OpenReview](https://openreview.net/forum?id=e0X9l4kecx) (Poster)

[16] Descent with Misaligned Gradients and Applications to Hidden Convexity. [OpenReview](https://openreview.net/forum?id=2L4PTJO8VQ) (Poster)

[17] DUET: Decentralized Bilevel Optimization without Lower-Level Strong Convexity. [OpenReview](https://openreview.net/forum?id=jxMAPMqNr5) (Poster)

[18] Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity. [OpenReview](https://openreview.net/forum?id=0wmfzWPAFu) (Poster)

*[19] On the Almost Sure Convergence of the Stochastic Three Points Algorithm. [OpenReview](https://openreview.net/forum?id=N8tJmhCw25) (Poster)

[20] Gradient correlation is needed to accelerate SGD with momentum. [OpenReview](https://openreview.net/forum?id=2Q8gTck8Uq) (Poster)  

[21] Last Iterate Convergence of Incremental Methods as a Model of Forgetting. [OpenReview](https://openreview.net/forum?id=mSGcDhQPwm) (Poster)

[22] OPTAMI: Global Superlinear Convergence of High-order Methods. [OpenReview](https://openreview.net/forum?id=Cpr6Wv2tfr) (Poster)

[23] Decentralized Optimization with Coupled Constraints. [OpenReview](https://openreview.net/forum?id=AJM52ygi6Y) (Poster)

[24] Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions. [OpenReview](https://openreview.net/forum?id=cyPMEXdqQ2) (Poster)

[25] Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization. [OpenReview](https://openreview.net/forum?id=TrJ36UfD9P) (Poster)

[26] Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance. [OpenReview](https://openreview.net/forum?id=nuX2yPejiL) (Poster)

[27] Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness. [OpenReview](https://openreview.net/forum?id=ZjOXuAfS6l) (Poster)

[29] Optimizing $(L_0, L_1)$-Smooth Functions by Gradient Methods. [OpenReview](https://openreview.net/forum?id=GQ1Tc3vHbt) (Poster)

[30] Sharpness Aware Minimization: General Analysis and Improved Rates. [OpenReview](https://openreview.net/forum?id=8rvqpiTTFv) (Poster)

[31] Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games. [OpenReview](https://openreview.net/forum?id=Jrt9iWalFy) (Poster)

[32] Accelerated Over-Relaxation Heavy-Ball Method: Achieving Global Accelerated Convergence with Broad Generalization. [OpenReview](https://openreview.net/forum?id=SWEqzy7IQB) (Poster)

[33] A Tight Convergence Analysis of Inexact Stochastic Proximal Point Algorithm for Stochastic Composite Optimization Problems. [OpenReview](https://openreview.net/forum?id=n3TkrH7fEr) (Poster)

[34] MixMax: Distributional Robustness in Function Space via Optimal Data Mixtures. [OpenReview](https://openreview.net/forum?id=dIkpHooa2D) (Poster)

[35] ADMM for Structured Fractional Minimization. [OpenReview](https://openreview.net/forum?id=DcZpQhVpp9) (Poster)

[36] Convergence of Distributed Adaptive Optimization with Local Updates. [OpenReview](https://openreview.net/forum?id=VNg7srnvD9) (Poster)

[37] SOREL: A Stochastic Algorithm for Spectral Risks Minimization. [OpenReview](https://openreview.net/forum?id=pdF86dyoS6) (Poster)

[38] Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression. [OpenReview](https://openreview.net/forum?id=lydPkW4lfz) (Poster)

[39] Tight Time Complexities in Parallel Stochastic Optimization with Arbitrary Computation Dynamics. [OpenReview](https://openreview.net/forum?id=cUN8lJB4rD) (Poster)

[40] Adaptive backtracking for fast optimization. [OpenReview](https://openreview.net/forum?id=SrGP0RQbYH) (Poster)

[41] AdaGrad under Anisotropic Smoothness: A Fine-Grained Analysis. [OpenReview](https://openreview.net/forum?id=4GT9uTsAJE) (Poster)

[42] Do Deep Neural Network Solutions Form a Star Domain?. [OpenReview](https://openreview.net/forum?id=QjO0fUlVYK) (Poster)

[43] Unlocking Global Optimality in Bilevel Optimization: A Pilot Study. [OpenReview](https://openreview.net/forum?id=2xvisNIfdw) (Poster)

[44] SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process. [OpenReview](https://openreview.net/forum?id=8HuLgtjqOD) (Poster)

[45] Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling. [OpenReview](https://openreview.net/forum?id=9EfBeXaXf0)

## Learning on Graphs

[1] Linkerhägner, J., Shi, C., & Dokmanić, I. (2024). Joint Graph Rewiring and Feature Denoising via Spectral Resonance. arXiv preprint arXiv:2408.07191. [OpenReview](https://openreview.net/forum?id=zBbZ2vdLzH) (Oral)