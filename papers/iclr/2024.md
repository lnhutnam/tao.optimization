## Game Theory

[1] Gemp, I., Marris, L., & Piliouras, G. (2023). Approximating nash equilibria in normal-form games via stochastic optimization. arXiv preprint arXiv:2310.06689. (Oral)

## Bayesian Deep Learning/ Bayesian Optimization

[1] Li, B., & Zhang, R. (2023). [Entropy-MCMC: Sampling from Flat Basins with Ease](https://openreview.net/forum?id=oGNdBvymod). arXiv preprint arXiv:2310.05401. (Poster)

[2] Sussex, S., Sessa, P. G., Makarova, A., & Krause, A. (2023). Adversarial Causal Bayesian Optimization. In The Twelfth International Conference on Learning Representations.

[3] Nguyen, Q. P., Chew, W. T. R., Song, L., Low, B. K. H., & Jaillet, P. (2023). Optimistic Bayesian Optimization with Unknown Constraints. In The Twelfth International Conference on Learning Representations.

[4] Tay, S. S., Foo, C. S., Urano, D., Leong, R., & Low, B. K. H. (2024, January). A Unified Framework for Bayesian Optimization under Contextual Uncertainty. In The Twelfth International Conference on Learning Representations.

[5] Li, Y. L., Rudner, T. G., & Wilson, A. G. (2023). A study of Bayesian neural network surrogates for Bayesian optimization. arXiv preprint arXiv:2305.20028.

[6] Bardou, A., Thiran, P., & Begin, T. (2023). Relaxing the additivity constraints in decentralized no-regret high-dimensional bayesian optimization. arXiv preprint arXiv:2305.19838.

[7] Bencomo, G. M., Snell, J. C., & Griffiths, T. L. (2023). Implicit maximum a posteriori filtering via adaptive optimization. arXiv preprint arXiv:2311.10580.

[8] Chanda, P., Modi, S., & Ramakrishnan, G. Bayesian Coreset Optimization for Personalized Federated Learning. In The Twelfth International Conference on Learning Representations.

[9] Mei, Y., Imani, M., & Lan, T. (2024). Bayesian optimization through Gaussian Cox process models for spatio-temporal data. arXiv preprint arXiv:2401.14544.

[10] Nguyen, Q. P., Low, B. K. H., & Jaillet, P. (2023). Meta-VBO: Utilizing Prior Tasks in Optimizing Risk Measures with Gaussian Processes. In The Twelfth International Conference on Learning Representations.

[11] Jain, N., Shanmugam, K., & Shenoy, P. (2024). Learning model uncertainty as variance-minimizing instance weights. In The Twelfth International Conference on Learning Representations.

[12] He, Z., Shu, Y., Dai, Z., & Low, B. K. H. (2024). Robustifying and Boosting Training-Free Neural Architecture Search. arXiv preprint arXiv:2403.07591.

## Black-Box Optimization

[1] Chen, J., Ma, Z., Guo, H., Ma, Y., Zhang, J., & Gong, Y. J. (2024). Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning. arXiv preprint arXiv:2402.02355.

*[2] Liu, X., Zhang, H., Gu, B., & Chen, H. [General Stability Analysis for Zeroth-Order Optimization Algorithms](https://openreview.net/forum?id=AfhNyr73Ma). In The Twelfth International Conference on Learning Representations. (Poster)

[3] Ye, F., Lyu, Y., Wang, X., Zhang, Y., & Tsang, I. [Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning](https://openreview.net/forum?id=bm1JVsVZVu). In The Twelfth International Conference on Learning Representations. (Poster)

[4] Nguyen, T., Wu, X., Dong, X., Nguyen, C. D. T., Ng, S. K., & Luu, A. T. (2024). Topic Modeling as Multi-Objective Contrastive Optimization. arXiv preprint arXiv:2402.07577.

[5] Ahn, K., Cheng, X., Song, M., Yun, C., Jadbabaie, A., & Sra, S. (2023). Linear attention is (maybe) all you need (to understand transformer optimization). arXiv preprint arXiv:2310.01082.

[6] Zhou, Z., Liu, L., Zhao, P., & Gong, W. Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution. In The Twelfth International Conference on Learning Representations.

[7] Yang, Y. R., Shi, C. W., & Li, W. J. [On the Effect of Batch Size in Byzantine-Robust Distributed Learning](https://openreview.net/forum?id=wriKDQqiOQ). In The Twelfth International Conference on Learning Representations. (Poster)

## Bi-level Optimization

[1] Yao, W., Yu, C., Zeng, S., & Zhang, J. (2024). [Constrained bi-level optimization: Proximal lagrangian value function approach and hessian-free algorithm](https://openreview.net/forum?id=xJ5N8qrEPl). arXiv preprint arXiv:2401.16164. (Spotlight)

[2] Kwon, J., Kwon, D., Wright, S., & Nowak, R. (2023). [On penalty methods for nonconvex bilevel optimization and first-order stochastic approximation](https://openreview.net/forum?id=CvYBvgEUK9). arXiv preprint arXiv:2309.01753. (Spotlight)

[3] Hao, J., Gong, X., & Liu, M. (2024). [Bilevel optimization under unbounded smoothness: A new algorithm and convergence analysis](https://openreview.net/forum?id=LqRGsGWOTX). arXiv preprint arXiv:2401.09587. (Spotlight)

[4] Sun, W., Zhang, X., Lu, H., Chen, Y., Wang, T., Chen, J., & Lin, L. (2024). Backdoor Contrastive Learning via Bi-level Trigger Optimization. arXiv preprint arXiv:2404.07863.

[5] Zhou, B., Jiang, R., & Shen, S. (2024). [Learning to solve bilevel programs with binary tender](https://openreview.net/forum?id=PsDFgTosqb). arXiv preprint arXiv:2407.16914. (Poster)

[6] Sow, D., Lin, S., Wang, Z., & Liang, Y. (2023). Doubly Robust Instance-Reweighted Adversarial Training. arXiv preprint arXiv:2308.00311.

[7] Ding, M., An, B., Xu, Y., Satheesh, A., & Huang, F. (2024, January). SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation. In The Twelfth International Conference on Learning Representations.

[8] Hu, C., Wu, H., Li, X., Ma, C., Chen, X., Yan, J., ... & Liu, X. (2023). Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation. arXiv preprint arXiv:2312.15112.

## Convex Optimization

*[1] Sahiner, A., Ergen, T., Ozturkler, B., Pauly, J. M., Mardani, M., & Pilanci, M. [Scaling Convex Neural Networks with Burer-Monteiro Factorization](https://openreview.net/forum?id=ikmuHqugN7). (Poster)

[4] Almasi, H., Mishra, H., Vamanan, B., & Ravi, S. N. (2023). [Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization](https://arxiv.org/abs/2302.05865). arXiv preprint arXiv:2302.05865. (Poster)

*[5] Baader, M., Müller, M. N., Mao, Y., & Vechev, M. (2023). [Expressivity of reLU-networks under convex relaxations](https://arxiv.org/abs/2311.04015). arXiv preprint arXiv:2311.04015. (Poster)

*[6] Sivan, H., Gabel, M., & Schuster, A. (2023). [FOSI: Hybrid First and Second Order Optimization](https://arxiv.org/abs/2302.08484). arXiv preprint arXiv:2302.08484. (Poster)

[7] Bai, S., & Bullins, B. [Local Composite Saddle Point Optimization](https://openreview.net/forum?id=kklwv4c4dI). In The Twelfth International Conference on Learning Representations. (Poster)

*[8] Liu, Z., & Zhou, Z. (2023). [Revisiting the last-iterate convergence of stochastic gradient methods](https://arxiv.org/abs/2312.08531). arXiv preprint arXiv:2312.08531. (Poster)

*[9] Elenter, J., Chamon, L. F., & Ribeiro, A. (2024). [Near-optimal solutions of constrained learning problems](https://arxiv.org/abs/2403.11844). arXiv preprint arXiv:2403.11844. (Poster)

[10] De Palma, A., Bunel, R., Dvijotham, K., Kumar, M. P., Stanforth, R., & Lomuscio, A. (2023). [Expressive Losses for Verified Robustness via Convex Combinations](https://openreview.net/forum?id=mzyZ4wzKlM). arXiv preprint arXiv:2305.13991. (Poster)

*[11] Tang, X., Shavlovsky, M., Rahmanian, H., Tardini, E., Thekumparampil, K. K., Xiao, T., & Ying, L. (2024). [Accelerating sinkhorn algorithm with sparse newton iterations](https://openreview.net/forum?id=Kuj5gVp5GQ). arXiv preprint arXiv:2401.12253. (Poster)

*[12] Agafonov, A., Kamzolov, D., Gasnikov, A., Kavis, A., Antonakopoulos, K., Cevher, V., & Takáč, M. (2023). [Advancing the lower bounds: An accelerated, stochastic, second-order method with optimal adaptation to inexactness](https://openreview.net/forum?id=otU31x3fus). arXiv preprint arXiv:2309.01570. (Poster)

[13] Adilova, L., Fischer, A., & Jaggi, M. (2023). Layerwise linear mode connectivity. arXiv preprint arXiv:2307.06966.

*[14] Wang, H., Zhang, C., & Li, T. (2024). [Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss](https://openreview.net/forum?id=pB1FeRSQxh). arXiv preprint arXiv:2402.12745. (Poster)

## Convergence 

[1] Zhao, B., Gower, R. M., Walters, R., & Yu, R. (2023). [Improving Convergence and Generalization Using Parameter Symmetries](https://openreview.net/forum?id=L0r0GphlIL). arXiv preprint arXiv:2305.13404. (Oral)

*[2] Robin, D. A., Scaman, K., & Lelarge, M. (2025). [Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks](https://openreview.net/forum?id=rBH7x87VfJ). arXiv preprint arXiv:2501.05930. (Poster)

## Differentiable Optimization

[1] Bambade, A., Schramm, F., Taylor, A., & Carpentier, J. [Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning](https://openreview.net/forum?id=YCPDFfmkFr). In The Twelfth International Conference on Learning Representations. (Spotlight)

[2] Chalapathi, N., Du, Y., & Krishnapriyan, A. (2024). Scaling physics-informed hard constraints with mixture-of-experts. arXiv preprint arXiv:2402.13412.

[3] Sun, Z., Suresh, A. T., & Menon, A. K. (2023). The importance of feature preprocessing for differentially private linear optimization. arXiv preprint arXiv:2307.11106.

*[4] Lin, Y., Ma, Y. A., Wang, Y. X., Redberg, R., & Bu, Z. (2023). [Tractable mcmc for private learning with pure and gaussian differential privacy](https://openreview.net/forum?id=pmweVpJ229). arXiv preprint arXiv:2310.14661. (Poster)

[5] Schnell, P., & Thuerey, N. (2024). Stabilizing Backpropagation Through Time to Learn Complex Physics. arXiv preprint arXiv:2405.02041.

## Distributed Optimization

[1] Gao, Y., Islamov, R., & Stich, S. (2023). [EControl: Fast Distributed Optimization with Compression and Error Control](https://openreview.net/forum?id=lsvlvWB9vz). arXiv preprint arXiv:2311.05645. (Poster)

[2] Chen, J., Ye, H., Wang, M., Huang, T., Dai, G., Tsang, I. W., & Liu, Y. (2023). [Decentralized Riemannian conjugate gradient method on the Stiefel manifold](https://openreview.net/forum?id=PQbFUMKLFp). arXiv preprint arXiv:2308.10547. (Poster)

[3] Adilova, L., Fischer, A., & Jaggi, M. (2023). Layerwise linear mode connectivity. arXiv preprint arXiv:2307.06966.

[4] Baharlouei, S., Patel, S., & Razaviyayn, M. (2023). f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization. arXiv preprint arXiv:2312.03259.

[5] Scott, J. A., Zakerinia, H., & Lampert, C. (2024). PeFLL: Personalized federated learning by learning to learn. In 12th International Conference on Learning Representations.

[6] Gu, X., Lyu, K., Arora, S., Zhang, J., & Huang, L. (2023). A Quadratic Synchronization Rule for Distributed Deep Learning. arXiv preprint arXiv:2310.14423.

## Discrete Optimization/ Mixed Linear Programming

[1] Kirjner, A., Yim, J., Samusevich, R., Bracha, S., Jaakkola, T. S., Barzilay, R., & Fiete, I. R. (2023). Improving protein optimization with smoothed fitness landscapes. In The Twelfth International Conference on Learning Representations.

*[2] Kuang, Y., Wang, J., Liu, H., Zhu, F., Li, X., Zeng, J., ... & Wu, F. (2024). Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework. In The Twelfth International Conference on Learning Representations.

[3] Hamman, F., & Dutta, S. (2023). Demystifying local and global fairness trade-offs in federated learning using partial information decomposition. arXiv preprint arXiv:2307.11333.

[4] Ye, H., Xu, H., & Wang, H. Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Lightweight Optimizer and Small-scale Training Dataset. In The Twelfth International Conference on Learning Representations.

[5] Cheng, H., Cong, Y., Jiang, W., & Pu, S. (2024). Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model. In The Twelfth International Conference on Learning Representations.

## Min-max Optimization

[1] [PILOT: An $O(1/K)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation](https://openreview.net/forum?id=OkHHJcMroY) (Spotlight)

[2] Zhang, S., Choudhury, S., Stich, S. U., & Loizou, N. (2023). Communication-efficient gradient descent-accent methods for distributed variational inequalities: Unified analysis and local updates. arXiv preprint arXiv:2306.05100.

[3] Fan, Y., Li, Y., & Chen, B. (2024). [Weaker MVI condition: Extragradient methods with multi-step exploration](https://openreview.net/forum?id=RNGUbTYSjk). In The Twelfth International Conference on Learning Representations. (Poster)

[4] Mavrothalassitis, I., Skoulakis, S., Dadi, L. T., & Cevher, V. (2024). [Efficient Continual Finite-Sum Minimization](https://openreview.net/forum?id=RR70yWYenC). arXiv preprint arXiv:2406.04731. (Poster)

[5] [57] Cai, X., Alacaoglu, A., & Diakonikolas, J. (2023). [Variance reduced halpern iteration for finite-sum monotone inclusions](https://openreview.net/forum?id=0i6Z9N5MLY). arXiv preprint arXiv:2310.02987. (Poster)

[6] Zhang, S., Song, Y., Yang, J., Li, Y., Han, B., & Tan, M. (2024). Detecting machine-generated texts by multi-population aware optimization for maximum mean discrepancy. arXiv preprint arXiv:2402.16041.

## Geometry Optimization

[17] Tsypin, A., Ugadiarov, L., Khrabrov, K., Telepov, A., Rumiantsev, E., Skrynnik, A., ... & Kadurin, A. (2023). Gradual optimization learning for conformational energy minimization. arXiv preprint arXiv:2311.06295.

## Lyapunov Analysis

[1] Chen, L., Liu, B., Liang, K., & Liu, Q. (2023). [Lion secretly solves constrained optimization: As lyapunov predicts](https://openreview.net/forum?id=e4xS9ZarDr). arXiv preprint arXiv:2310.05898. (Spotlight)


## Non-convex optimization

*[1] Li, H., Karagulyan, A., & Richtárik, P. (2023). [Det-CGD: Compressed gradient descent with matrix stepsizes for non-convex optimization](https://arxiv.org/abs/2305.12568). arXiv preprint arXiv:2305.12568. (Poster)

[2] Liang, E., & Chen, M. (2024). [Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping](https://openreview.net/forum?id=3tM1l5tSbv). In The Twelfth International Conference on Learning Representations. (Poster)

*[3] Pinilla, S., & Thiyagalingam, J. [Global Optimality for Non-linear Constrained Restoration Problems via Invexity](https://openreview.net/forum?id=fyTPWfXtcc). In The Twelfth International Conference on Learning Representations. (Poster)

*[4] Guan, J., & Xiong, H. (2024, March). [Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning](https://openreview.net/forum?id=pA8Q5WiEMg). In The Twelfth International Conference on Learning Representations. (Poster)

[5] Zhang, Q., Tran, H., & Cutkosky, A. (2024). Private zeroth-order nonsmooth nonconvex optimization. arXiv preprint arXiv:2406.19579.

[6] Guan, Z., Zhou, Y., & Liang, Y. (2024, February). On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback. In The Twelfth International Conference on Learning Representations.

[7] Murata, T., Niwa, K., Fukami, T., & Tyou, I. Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity. In The Twelfth International Conference on Learning Representations.

[8] Mangoubi, O., & Vishnoi, N. K. (2024). Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers. arXiv preprint arXiv:2409.04320.

## Stochastic Optimization

[1] Gemp, I., Marris, L., & Piliouras, G. [Approximating Nash Equilibria in Normal-Form Games via Unbiased Stochastic Optimization](https://openreview.net/forum?id=cc8h3I3V4E). (Oral)

[2] Mehta, R., Roulet, V., Pillutla, K., & Harchaoui, Z. (2023). [Distributionally robust optimization with bias and variance reduction](https://openreview.net/forum?id=TTrzgEZt9s). arXiv preprint arXiv:2310.13863. (Spotlight)

[3] Hu, J., Doshi, V., & Eun, D. Y. (2024). [Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks](https://openreview.net/forum?id=BV1PHbTJzd). arXiv preprint arXiv:2401.09665. (Oral)

[4] Gemp, I., Marris, L., & Piliouras, G. [Approximating Nash Equilibria in Normal-Form Games via Unbiased Stochastic Optimization](https://openreview.net/forum?id=cc8h3I3V4E). (Oral)

[5] Zhang, D., Chen, R. T., Liu, C. H., Courville, A., & Bengio, Y. (2023). Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. arXiv preprint arXiv:2310.02679.

[6] Richardson, N., Oktay, D., Ovadia, Y., Bowden, J. C., & Adams, R. P. (2024). Fiber monte carlo. In 12th International Conference on Learning Representations, ICLR 2024.

[7] Pedramfar, M., Nadew, Y. Y., Quinn, C. J., & Aggarwal, V. (2024). Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization. arXiv preprint arXiv:2403.10063.

[8] Ustimenko, A., & Beznosikov, A. (2023). Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting. arXiv preprint arXiv:2310.06081.

[9] Chen, Y., & Mauch, L. (2023). Order-preserving gflownets. arXiv preprint arXiv:2310.00386.

## Semidefinite Programming

[1] Zhuang, Y., Chen, X., Yang, Y., & Zhang, R. Y. (2023). [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming](https://openreview.net/forum?id=v7ZPwoHU1j). arXiv preprint arXiv:2305.18436. (Oral)

[2] Wang, Z., Hu, B., Havens, A. J., Araujo, A., Zheng, Y., Chen, Y., & Jha, S. (2024). On the scalability and memory efficiency of semidefinite programs for Lipschitz constant estimation of neural networks. In The Twelfth International Conference on Learning Representations.

## SGD/ First-Order/ Second-Order/ Adaptive Gradient Methods

[1] Li, J., Huang, F., & Huang, H. FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization. In The Twelfth International Conference on Learning Representations.

*[2] Wang, R., Malladi, S., Wang, T., Lyu, K., & Li, Z. (2023). [The marginal value of momentum for small learning rate sgd](https://openreview.net/forum?id=3JjJezzVkT). arXiv preprint arXiv:2307.15196. (Poster)

[3] Li, X., Deng, Y., Wu, J., Zhou, D., & Gu, Q. (2023). R[isk Bounds of Accelerated SGD for Overparameterized Linear Regression](https://openreview.net/forum?id=AcoXPIPh4A). arXiv preprint arXiv:2311.14222. (Poster)

[4] Ishikawa, S., & Karakida, R. (2023). On the Parameterization of Second-Order Optimization Effective Towards the Infinite Width. arXiv preprint arXiv:2312.12226.

[5] Rosenfeld, E., & Risteski, A. (2023). Outliers with opposing signals have an outsized effect on neural network optimization. arXiv preprint arXiv:2311.04163.

*[6] Choquette-Choo, C. A., Dvijotham, K., Pillutla, K., Ganesh, A., Steinke, T., & Thakurta, A. (2023). [Correlated noise provably beats independent noise for differentially private learning](https://openreview.net/forum?id=xHmCdSArUC). arXiv preprint arXiv:2310.06771. (Poster)

[7] Block, A., Foster, D. J., Krishnamurthy, A., Simchowitz, M., & Zhang, C. (2023). [Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression](https://openreview.net/forum?id=CgPs04l9TO). arXiv preprint arXiv:2310.11428. (Poster)

[8] Li, Z., Wang, Y., & Wang, Z. Fast Equilibrium of SGD in Generic Situations. In The Twelfth International Conference on Learning Representations.

[9] Dexter, G., Ocejo, B., Keerthi, S., Gupta, A., Acharya, A., & Khanna, R. (2024). A precise characterization of sgd stability using loss surface geometry. arXiv preprint arXiv:2401.12332.

## Robust Optimization

[1] Dumouchelle, J., Julien, E., Kurtz, J., & Khalil, E. B. (2023). Neur2RO: Neural two-stage robust optimization. In The Twelfth International Conference on Learning Representations.

*[2] Grari, V., Laugel, T., Hashimoto, T., Lamprier, S., & Detyniecki, M. (2023). On the Fairness ROAD: Robust Optimization for Adversarial Debiasing. arXiv preprint arXiv:2310.18413.

[3] Cristian, R. C., & Perakis, G. A Discretization Framework for Robust Contextual Stochastic Optimization. In The Twelfth International Conference on Learning Representations.

[4] Ghaffari, S., Saleh, E., Schwing, A. G., Wang, Y. X., Burke, M. D., & Sinha, S. (2023). Robust Model-Based Optimization for Challenging Fitness Landscapes. arXiv preprint arXiv:2305.13650.

[5] Theodoropoulos, P., Liu, G. H., Chen, T., Saravanos, A. D., & Theodorou, E. A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER. In The Twelfth International Conference on Learning Representations.

[6] Shin, S., Bae, H., Na, B., Kim, Y. Y., & Moon, I. C. (2024). Unknown Domain Inconsistency Minimization for Domain Generalization. arXiv preprint arXiv:2403.07329.

[7] Chhabra, A., Li, P., Mohapatra, P., & Liu, H. (2024). " What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection. In The Twelfth International Conference on Learning Representations. (Oral)

## Preference Optimization

[27] Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., & Liu, J. (2023). Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657.

## Zeroth-Order Optimization

[1] Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J., Parasyris, K., ... & Liu, S. (2023). Deepzero: Scaling up zeroth-order optimization for deep model training. arXiv preprint arXiv:2310.02025.

[2] Tang, Z., Rybin, D., & Chang, T. H. (2023). Zeroth-order optimization meets human feedback: Provable learning via ranking oracles. arXiv preprint arXiv:2303.03751.

[3] Yuan, X., de Vazelhes, W., Gu, B., & Xiong, H. New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions. In The Twelfth International Conference on Learning Representations.



## Applications


### Causal

[1] Zuo, A., Li, Y., Wei, S., & Gong, M. (2024). Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach. arXiv preprint arXiv:2401.10632.

[2] Saboksayr, Seyed Saman, Gonzalo Mateos, and Mariano Tepper. ["CoLiDE: Concomitant Linear DAG Estimation."](https://openreview.net/forum?id=fGAIgO75dG) arXiv preprint arXiv:2310.02895 (2023). (Poster)

[3] Massidda, R., Landolfi, F., Cinquini, M., & Bacciu, D. (2023). Constraint-Free Structure Learning with Smooth Acyclic Orientations. arXiv preprint arXiv:2309.08406.

[4] Seng, J., Zečević, M., Dhami, D. S., & Kersting, K. (2024). Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG. In The Twelfth International Conference on Learning Representations.

### Data Selection, Interpretability, Fairness, Robustness

[1] Chhabra, A., Li, P., Mohapatra, P., & Liu, H. (2024). [" What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection](https://openreview.net/forum?id=HE9eUQlAvo). In The Twelfth International Conference on Learning Representations. (Oral)

###  Diffusion Models 

[1] Zheng, X., Pang, T., Du, C., Jiang, J., & Lin, M. (2023). [Intriguing properties of data attribution on diffusion models](https://openreview.net/forum?id=vKViCoKGcB). arXiv preprint arXiv:2311.00500. (Poster)

[2] Huang, Y., Wang, J., Shi, Y., Tang, B., Qi, X., & Zhang, L. (2023). Dreamtime: An improved optimization strategy for diffusion-guided 3d generation. In The Twelfth International Conference on Learning Representations.

[3] Zhou, X., Cheng, X., Yang, Y., Bao, Y., Wang, L., & Gu, Q. (2024). DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization. arXiv preprint arXiv:2403.13829.

[4] Chen, H., Lu, C., Wang, Z., Su, H., & Zhu, J. (2023). Score regularized policy optimization through diffusion behavior. arXiv preprint arXiv:2310.07297.

[5] Han, Y., Razaviyayn, M., & Xu, R. (2024). Neural network-based score estimation in diffusion models: Optimization and generalization. arXiv preprint arXiv:2401.15604.

[6] Lukas, N., Diaa, A., Fenaux, L., & Kerschbaum, F. (2023). Leveraging optimization for adaptive attacks on image watermarks. arXiv preprint arXiv:2309.16952.

[7] Gu, J., Zhai, S., Zhang, Y., Susskind, J. M., & Jaitly, N. (2023, October). Matryoshka diffusion models. In The Twelfth International Conference on Learning Representations.

[8] Shi, D., Tong, Y., Zhou, Z., Xu, K., Wang, Z., & Ye, J. GRAPH-CONSTRAINED DIFFUSION FOR END-TO-END PATH PLANNING. In The Twelfth International Conference on Learning Representations.

[9] Li, Y., & van der Schaar, M. (2023). On Error Propagation of Diffusion Models. In The Twelfth International Conference on Learning Representations.

[10] Zhang, G., Kenta, N., & Kleijn, W. B. (2023). On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. arXiv preprint arXiv:2304.11328.

[11] Wizadwongsa, S., Chinchuthakun, W., Khungurn, P., Raj, A., & Suwajanakorn, S. (2023). Diffusion Sampling with Momentum for Mitigating Divergence Artifacts. arXiv preprint arXiv:2307.11118.

[12] Mardani, M., Song, J., Kautz, J., & Vahdat, A. (2023). A variational perspective on solving inverse problems with diffusion models. arXiv preprint arXiv:2305.04391.

[13] Chung, H., Lee, S., & Ye, J. C. (2023). Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems. arXiv preprint arXiv:2303.05754.

### Hyperbolic Space

[1] Chlenski, P., Turok, E., Moretti, A., & Pe'er, I. (2023). Fast hyperboloid decision tree algorithms. arXiv preprint arXiv:2310.13841.

[2] Yu, T., Liu, T. J., Tseng, A., & De Sa, C. (2023). Shadow Cones: A Generalized Framework for Partial Order Embeddings. arXiv preprint arXiv:2305.15215.

### Mixup

[1] Fisher, Q., Meng, H., & Papyan, V. (2024). [Pushing Boundaries: Mixup's Influence on Neural Collapse](https://openreview.net/forum?id=jTSKkcbEsj). arXiv preprint arXiv:2402.06171. (Poster)

### Metric Learning 

[1] Xu, A., Hsieh, J. Y., Vundurthy, B., Cohen, E., Choset, H., & Li, L. (2022). Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem. arXiv preprint arXiv:2210.11173.

### Neural Bandits/ Bandit Optimization

[1] Deb, R., Ban, Y., Zuo, S., He, J., & Banerjee, A. (2023). [Contextual bandits with online neural regression](https://openreview.net/forum?id=5ep85sakT3). arXiv preprint arXiv:2312.07145. (Poster)

[2] Li, C., Liu, C., & Wang, Y. X. (2023). Communication-Efficient Federated Non-Linear Bandit Optimization. arXiv preprint arXiv:2311.01695.

[3] Lu, Z., Zhang, Q., Chen, X., Zhang, F., Woodruff, D., & Hazan, E. (2024). [Adaptive Regret for Bandits Made Possible: Two Queries Suffice](https://openreview.net/forum?id=AY9KyTGcnk). arXiv preprint arXiv:2401.09278. (Poster)

### Reinforcement Learning

[1] Valensi, D., Derman, E., Mannor, S., & Dalal, G. (2024). Tree Search-Based Policy Optimization under Stochastic Execution Delay. arXiv preprint arXiv:2404.05440.

[2] Sikchi, H., Chitnis, R., Touati, A., Geramifard, A., Zhang, A., & Niekum, S. (2023). [Score models for offline goal-conditioned reinforcement learning](https://openreview.net/forum?id=oXjnwQLcTA). arXiv preprint arXiv:2311.02013. (Poster)

[3] Xiong, N., Liu, Z., Wang, Z., & Yang, Z. (2023). Sample-efficient multi-agent rl: An optimization perspective. arXiv preprint arXiv:2310.06243.

[4] Sun, H., Hüyük, A., & van der Schaar, M. (2023). Query-dependent prompt evaluation and optimization with offline inverse RL. In The Twelfth International Conference on Learning Representations.

[5] Ju, P., Ghosh, A., & Shroff, N. [Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning](https://openreview.net/forum?id=yoVq2BGQdP). In The Twelfth International Conference on Learning Representations. (Poster)

[6] Schneider, J., Schumacher, P., Guist, S., Chen, L., Häufle, D., Schölkopf, B., & Büchler, D. (2024). Identifying Policy Gradient Subspaces. arXiv preprint arXiv:2401.06604.

[7] Chakraborty, S., Bedi, A. S., Koppel, A., Manocha, D., Wang, H., Wang, M., & Huang, F. (2023). PARL: A unified framework for policy alignment in reinforcement learning. arXiv preprint arXiv:2308.02585, 3.

[8] Dai, Z., Tomasi, F., & Ghiassian, S. (2024). In-context Exploration-Exploitation for Reinforcement Learning. arXiv preprint arXiv:2403.06826.

[9] Gomez, D., Bowling, M., & Machado, M. C. (2023). Proper Laplacian Representation Learning. arXiv preprint arXiv:2310.10833.

[10] Lei, K., He, Z., Lu, C., Hu, K., Gao, Y., & Xu, H. (2023). Uni-o4: Unifying online and offline deep reinforcement learning with multi-step on-policy optimization. arXiv preprint arXiv:2311.03351.

[11] Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S., Knox, W. B., & Sadigh, D. (2023). Contrastive prefence learning: Learning from human feedback without rl. arXiv preprint arXiv:2310.13639.

[12] Jin, R., Li, S., & Wang, B. On Stationary Point Convergence of PPO-Clip. In The Twelfth International Conference on Learning Representations.

[13] Liu, S., & Zhu, M. (2023). Meta inverse constrained reinforcement learning: Convergence guarantee and generalization analysis. In The Twelfth International Conference on Learning Representations.

[14] Ni, T., Eysenbach, B., Seyedsalehi, E., Ma, M., Gehring, C., Mahajan, A., & Bacon, P. L. (2024). Bridging State and History Representations: Understanding Self-Predictive RL. arXiv preprint arXiv:2401.08898.

[15] Castanyer, R. C., Romoff, J., & Berseth, G. (2023). Improving Intrinsic Exploration by Creating Stationary Objectives. arXiv preprint arXiv:2310.18144.

[16] Pan, H. R., & Schölkopf, B. (2024). Skill or luck? return decomposition via advantage functions. arXiv preprint arXiv:2402.12874.

[17] Black, K., Janner, M., Du, Y., Kostrikov, I., & Levine, S. (2023). Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301.

[18] Zhang, H., Lei, Y., Gui, L., Yang, M., He, Y., Wang, H., & Xu, R. (2024). CPPO: Continual Learning for Reinforcement Learning with Human Feedback. In The Twelfth International Conference on Learning Representations.

[19] Guo, J., & Schwaller, P. (2023). Beam enumeration: probabilistic explainability for sample efficient self-conditioned molecular design. arXiv preprint arXiv:2309.13957.

[20] Jia, C., Gao, C., Yin, H., Zhang, F., Chen, X. H., Xu, T., ... & Yu, Y. (2024). Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning. In The Twelfth International Conference on Learning Representations.

[21] Wu, Z., Tang, B., Lin, Q., Yu, C., Mao, S., Xie, Q., ... & Wang, D. (2024). [Off-Policy Primal-Dual Safe Reinforcement Learning](https://openreview.net/forum?id=vy42bYs1Wo). arXiv preprint arXiv:2401.14758. (Poster)

[22] Tiboni, G., Klink, P., Peters, J., Tommasi, T., D'Eramo, C., & Chalvatzaki, G. (2023). Domain Randomization via Entropy Maximization. arXiv preprint arXiv:2311.01885.

[23] Kallel, M., Basu, D., Akrour, R., & d'Eramo, C. (2024). Augmented bayesian policy search. arXiv preprint arXiv:2407.04864.

[24] Hu, S., Shen, L., Zhang, Y., & Tao, D. (2024). Learning multi-agent communication from graph modeling perspective. arXiv preprint arXiv:2405.08550.

[25] He, Q., Zhou, T., Fang, M., & Maghsudi, S. (2024). Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation. arXiv preprint arXiv:2404.12754.

[26] Carvalho, T. H., Tjhia, K., & Lelis, L. H. (2024). Reclaiming the source of programmatic policies: Programmatic versus latent spaces. arXiv preprint arXiv:2410.12166.

[27] Liu, Q., Ye, J., Ma, X., Yang, J., Liang, B., & Zhang, C. (2024). Efficient Multi-agent Reinforcement Learning by Planning. arXiv preprint arXiv:2405.11778.

[28] Ma, Y. J., Liang, W., Wang, G., Huang, D. A., Bastani, O., Jayaraman, D., ... & Anandkumar, A. (2023). Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931.

[30] Sengupta, A., Dixit, S., Akhtar, M. S., & Chakraborty, T. A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation. In The Twelfth International Conference on Learning Representations.

[31] Li, Y., Ju, P., & Shroff, N. (2023). Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping. arXiv preprint arXiv:2306.12981.

[32] Zhou, Y., Sekhari, A., Song, Y., & Sun, W. (2023). Offline data enhanced on-policy policy gradient with provable guarantees. arXiv preprint arXiv:2311.08384.

[33] He, J., Zhong, H., & Yang, Z. (2024). Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation. arXiv preprint arXiv:2404.12648.

[34] Zheng, Y., Li, J., Yu, D., Yang, Y., Li, S. E., Zhan, X., & Liu, J. (2024). Safe offline reinforcement learning with feasibility-guided diffusion model. arXiv preprint arXiv:2401.10700.

[35] Zhang, Z., Sun, Y., Ye, J., Liu, T. S., Zhang, J., & Yu, Y. (2023). Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation. In The Twelfth International Conference on Learning Representations.

[36] Subramani, R., Williams, M., Heitmann, M., Holm, H., Griffin, C., & Skalse, J. (2023). On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning. arXiv preprint arXiv:2310.11840.

[37] Jackson, M. T., Lu, C., Kirsch, L., Lange, R. T., Whiteson, S., & Foerster, J. N. (2024). Discovering temporally-aware reinforcement learning algorithms. arXiv preprint arXiv:2402.05828.

[38] Shen, L., Chen, S., Song, L., Jin, L., Peng, B., Mi, H., ... & Yu, D. (2023). [The trickle-down impact of reward (in-) consistency on rlhf](https://openreview.net/forum?id=MeHmwCDifc). arXiv preprint arXiv:2309.16155. (Poster)