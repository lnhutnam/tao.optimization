## Machine Learning

[1] Chhabra, A., Li, P., Mohapatra, P., & Liu, H. (2024). " What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection. In The Twelfth International Conference on Learning Representations. [OpenReview](https://openreview.net/forum?id=HE9eUQlAvo) (Oral)

[2] Gemp, I., Marris, L., & Piliouras, G. (2023). Approximating nash equilibria in normal-form games via stochastic optimization. arXiv preprint arXiv:2310.06689. [OpenReview](https://openreview.net/forum?id=cc8h3I3V4E) (Oral)

[3] Everaert, D., & Potts, C. (2023). Gio: Gradient information optimization for training dataset selection. arXiv preprint arXiv:2306.11670. [OpenReview](https://openreview.net/forum?id=3NnfJnbJT2) (Spotlight)

[4] Wang, C., Jiang, Y., Yang, C., Liu, H., & Chen, Y. (2023). Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240. [OpenReview](https://openreview.net/forum?id=2cRzmWXK9N) (Spotlight) 

[5] Zhang, Y., Yu, H., Li, J., & Lin, W. Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression. In The Twelfth International Conference on Learning Representations. [OpenReview](https://openreview.net/forum?id=D5mJSNtUtv) (Spotlight) 

[6] Xu, H., Li, K., Fu, H., Fu, Q., Xing, J., & Cheng, J. (2024). Dynamic discounted counterfactual regret minimization. In The Twelfth International Conference on Learning Representations. [OpenReview](https://openreview.net/forum?id=6PbvbLyqT6) (Spotlight) 

[7] Li, D., Ling, C., Xiong, H., & Gu, B. (2024, January). Learning No-Regret Sparse Generalized Linear Models with Varying Observation (s). In The Twelfth International Conference on Learning Representations. [OpenReview](https://openreview.net/forum?id=wISvONp3Kq)

[8] Liang, E., & Chen, M. (2024). [Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping](https://openreview.net/forum?id=3tM1l5tSbv). In The Twelfth International Conference on Learning Representations. (Poster)

[9] Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., & Liu, J. (2023). Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657.

[10] Chanda, P., Modi, S., & Ramakrishnan, G. Bayesian Coreset Optimization for Personalized Federated Learning. In The Twelfth International Conference on Learning Representations.

[11] Jain, N., Shanmugam, K., & Shenoy, P. (2024). Learning model uncertainty as variance-minimizing instance weights. In The Twelfth International Conference on Learning Representations.

[12] He, Z., Shu, Y., Dai, Z., & Low, B. K. H. (2024). Robustifying and Boosting Training-Free Neural Architecture Search. arXiv preprint arXiv:2403.07591.

[13] Kuang, Y., Wang, J., Liu, H., Zhu, F., Li, X., Zeng, J., ... & Wu, F. (2024). Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework. In The Twelfth International Conference on Learning Representations.

[14] Cheng, H., Cong, Y., Jiang, W., & Pu, S. (2024). Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model. In The Twelfth International Conference on Learning Representations.

[15] Gemp, I., Marris, L., & Piliouras, G. [Approximating Nash Equilibria in Normal-Form Games via Unbiased Stochastic Optimization](https://openreview.net/forum?id=cc8h3I3V4E). (Oral)

[16] Li, J., Huang, F., & Huang, H. FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization. In The Twelfth International Conference on Learning Representations.

## Optimization

[1] Zhao, B., Gower, R. M., Walters, R., & Yu, R. (2023). Improving Convergence and Generalization Using Parameter Symmetries. arXiv preprint arXiv:2305.13404. [OpenReview](https://openreview.net/forum?id=L0r0GphlIL) (Oral)

[2] Hu, J., Doshi, V., & Eun, D. Y. (2024). [Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks](https://openreview.net/forum?id=BV1PHbTJzd). arXiv preprint arXiv:2401.09665. (Oral)

[3] Zhuang, Y., Chen, X., Yang, Y., & Zhang, R. Y. (2023). [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming](https://openreview.net/forum?id=v7ZPwoHU1j). arXiv preprint arXiv:2305.18436. (Oral)

[4] Yao, W., Yu, C., Zeng, S., & Zhang, J. (2024). [Constrained bi-level optimization: Proximal lagrangian value function approach and hessian-free algorithm](https://openreview.net/forum?id=xJ5N8qrEPl). arXiv preprint arXiv:2401.16164. (Spotlight)

[5] [PILOT: An $O(1/K)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation](https://openreview.net/forum?id=OkHHJcMroY) (Spotlight)

[6] Bambade, A., Schramm, F., Taylor, A., & Carpentier, J. [Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning](https://openreview.net/forum?id=YCPDFfmkFr). In The Twelfth International Conference on Learning Representations. (Spotlight)

[7] Mehta, R., Roulet, V., Pillutla, K., & Harchaoui, Z. (2023). [Distributionally robust optimization with bias and variance reduction](https://openreview.net/forum?id=TTrzgEZt9s). arXiv preprint arXiv:2310.13863. (Spotlight)

[8] Chen, L., Liu, B., Liang, K., & Liu, Q. (2023). [Lion secretly solves constrained optimization: As lyapunov predicts](https://openreview.net/forum?id=e4xS9ZarDr). arXiv preprint arXiv:2310.05898. (Spotlight)

[9] Kwon, J., Kwon, D., Wright, S., & Nowak, R. (2023). [On penalty methods for nonconvex bilevel optimization and first-order stochastic approximation](https://openreview.net/forum?id=CvYBvgEUK9). arXiv preprint arXiv:2309.01753. (Spotlight)

[10] Hao, J., Gong, X., & Liu, M. (2024). [Bilevel optimization under unbounded smoothness: A new algorithm and convergence analysis](https://openreview.net/forum?id=LqRGsGWOTX). arXiv preprint arXiv:2401.09587. (Spotlight)

[11] Sherborne, T., Saphra, N., Dasigi, P., & Peng, H. (2023). TRAM: Bridging trust regions and sharpness aware minimization. arXiv preprint arXiv:2310.03646. [OpenReview](https://openreview.net/forum?id=kxebDHZ7b7) (Spotlight)

[12] Yu, P., Li, J., & Huang, H. (2023). Dropout enhanced bilevel training. In The Twelfth International Conference on Learning Representations. [OpenReview](https://openreview.net/forum?id=06lrITXVAx) (Spotlight)

[13] Kim, J., Yamamoto, K., Oko, K., Yang, Z., & Suzuki, T. (2023). Symmetric mean-field langevin dynamics for distributional minimax problems. arXiv preprint arXiv:2312.01127. [OpenReview](https://openreview.net/forum?id=YItWKZci78) (Spotlight)

[14] Sahiner, A., Ergen, T., Ozturkler, B., Pauly, J. M., Mardani, M., & Pilanci, M. [Scaling Convex Neural Networks with Burer-Monteiro Factorization](https://openreview.net/forum?id=ikmuHqugN7). (Poster)

[15] Elenter, J., Chamon, L. F., & Ribeiro, A. (2024). [Near-optimal solutions of constrained learning problems](https://arxiv.org/abs/2403.11844). arXiv preprint arXiv:2403.11844. (Poster)

[16] Liu, Z., & Zhou, Z. (2023). [Revisiting the last-iterate convergence of stochastic gradient methods](https://arxiv.org/abs/2312.08531). arXiv preprint arXiv:2312.08531. (Poster)

[17] Baader, M., Müller, M. N., Mao, Y., & Vechev, M. (2023). [Expressivity of reLU-networks under convex relaxations](https://arxiv.org/abs/2311.04015). arXiv preprint arXiv:2311.04015. (Poster)

[18] Wang, H., Zhang, C., & Li, T. (2024). [Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss](https://openreview.net/forum?id=pB1FeRSQxh). arXiv preprint arXiv:2402.12745. (Poster)

[19] Li, H., Karagulyan, A., & Richtárik, P. (2023). [Det-CGD: Compressed gradient descent with matrix stepsizes for non-convex optimization](https://arxiv.org/abs/2305.12568). arXiv preprint arXiv:2305.12568. (Poster)

[20] De Palma, A., Bunel, R., Dvijotham, K., Kumar, M. P., Stanforth, R., & Lomuscio, A. (2023). [Expressive Losses for Verified Robustness via Convex Combinations](https://openreview.net/forum?id=mzyZ4wzKlM). arXiv preprint arXiv:2305.13991. (Poster)

[21] Almasi, H., Mishra, H., Vamanan, B., & Ravi, S. N. (2023). [Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization](https://arxiv.org/abs/2302.05865). arXiv preprint arXiv:2302.05865. (Poster)

[22] Pinilla, S., & Thiyagalingam, J. [Global Optimality for Non-linear Constrained Restoration Problems via Invexity](https://openreview.net/forum?id=fyTPWfXtcc). In The Twelfth International Conference on Learning Representations. (Poster)

[23] Ye, F., Lyu, Y., Wang, X., Zhang, Y., & Tsang, I. [Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning](https://openreview.net/forum?id=bm1JVsVZVu). In The Twelfth International Conference on Learning Representations. (Poster)

[24] Chen, J., Ye, H., Wang, M., Huang, T., Dai, G., Tsang, I. W., & Liu, Y. (2023). [Decentralized Riemannian conjugate gradient method on the Stiefel manifold](https://openreview.net/forum?id=PQbFUMKLFp). arXiv preprint arXiv:2308.10547. (Poster)

[25] Gao, Y., Islamov, R., & Stich, S. (2023). [EControl: Fast Distributed Optimization with Compression and Error Control](https://openreview.net/forum?id=lsvlvWB9vz). arXiv preprint arXiv:2311.05645. (Poster)

[26] Sivan, H., Gabel, M., & Schuster, A. (2023). [FOSI: Hybrid First and Second Order Optimization](https://arxiv.org/abs/2302.08484). arXiv preprint arXiv:2302.08484. (Poster)

[27] Tang, X., Shavlovsky, M., Rahmanian, H., Tardini, E., Thekumparampil, K. K., Xiao, T., & Ying, L. (2024). [Accelerating sinkhorn algorithm with sparse newton iterations](https://openreview.net/forum?id=Kuj5gVp5GQ). arXiv preprint arXiv:2401.12253. (Poster)

[28] Wang, R., Malladi, S., Wang, T., Lyu, K., & Li, Z. (2023). [The marginal value of momentum for small learning rate sgd](https://openreview.net/forum?id=3JjJezzVkT). arXiv preprint arXiv:2307.15196. (Poster)

[29] Agafonov, A., Kamzolov, D., Gasnikov, A., Kavis, A., Antonakopoulos, K., Cevher, V., & Takáč, M. (2023). [Advancing the lower bounds: An accelerated, stochastic, second-order method with optimal adaptation to inexactness](https://openreview.net/forum?id=otU31x3fus). arXiv preprint arXiv:2309.01570. (Poster)

[30] Fan, Y., Li, Y., & Chen, B. (2024). [Weaker MVI condition: Extragradient methods with multi-step exploration](https://openreview.net/forum?id=RNGUbTYSjk). In The Twelfth International Conference on Learning Representations. (Poster)

[31] Bai, S., & Bullins, B. [Local Composite Saddle Point Optimization](https://openreview.net/forum?id=kklwv4c4dI). In The Twelfth International Conference on Learning Representations. (Poster)

[32] Zhou, B., Jiang, R., & Shen, S. (2024). [Learning to solve bilevel programs with binary tender](https://openreview.net/forum?id=PsDFgTosqb). arXiv preprint arXiv:2407.16914. (Poster)

[33] Mavrothalassitis, I., Skoulakis, S., Dadi, L. T., & Cevher, V. (2024). [Efficient Continual Finite-Sum Minimization](https://openreview.net/forum?id=RR70yWYenC). arXiv preprint arXiv:2406.04731. (Poster)

[34] Cai, X., Alacaoglu, A., & Diakonikolas, J. (2023). [Variance reduced halpern iteration for finite-sum monotone inclusions](https://openreview.net/forum?id=0i6Z9N5MLY). arXiv preprint arXiv:2310.02987. (Poster)

[35] Deb, R., Ban, Y., Zuo, S., He, J., & Banerjee, A. (2023). [Contextual bandits with online neural regression](https://openreview.net/forum?id=5ep85sakT3). arXiv preprint arXiv:2312.07145. (Poster)

[36] Yang, Y. R., Shi, C. W., & Li, W. J. [On the Effect of Batch Size in Byzantine-Robust Distributed Learning](https://openreview.net/forum?id=wriKDQqiOQ). In The Twelfth International Conference on Learning Representations. (Poster)

[37] Zhang, Q., Tran, H., & Cutkosky, A. (2024). Private zeroth-order nonsmooth nonconvex optimization. arXiv preprint arXiv:2406.19579.

[38] Ahn, K., Cheng, X., Song, M., Yun, C., Jadbabaie, A., & Sra, S. (2023). Linear attention is (maybe) all you need (to understand transformer optimization). arXiv preprint arXiv:2310.01082.

[39] Duvvuri, S. S., Devvrit, F., Anil, R., Hsieh, C. J., & Dhillon, I. S. (2024). Combining axes preconditioners through kronecker approximation for deep learning. In The Twelfth International Conference on Learning Representations.

[40] Zhu, L., Liu, C., Radhakrishnan, A., & Belkin, M. (2022). Quadratic models for understanding catapult dynamics of neural networks. arXiv preprint arXiv:2205.11787.

[41] Dumouchelle, J., Julien, E., Kurtz, J., & Khalil, E. B. (2023). Neur2RO: Neural two-stage robust optimization. In The Twelfth International Conference on Learning Representations.

[42] Cristian, R. C., & Perakis, G. A Discretization Framework for Robust Contextual Stochastic Optimization. In The Twelfth International Conference on Learning Representations.

[43] Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J., Parasyris, K., ... & Liu, S. (2023). Deepzero: Scaling up zeroth-order optimization for deep model training. arXiv preprint arXiv:2310.02025.

[44] Ishikawa, S., & Karakida, R. (2023). On the Parameterization of Second-Order Optimization Effective Towards the Infinite Width. arXiv preprint arXiv:2312.12226.

[45] Rosenfeld, E., & Risteski, A. (2023). Outliers with opposing signals have an outsized effect on neural network optimization. arXiv preprint arXiv:2311.04163.

[46]  Li, Z., Wang, Y., & Wang, Z. Fast Equilibrium of SGD in Generic Situations. In The Twelfth International Conference on Learning Representations.

[47] Zhang, S., Choudhury, S., Stich, S. U., & Loizou, N. (2023). Communication-efficient gradient descent-accent methods for distributed variational inequalities: Unified analysis and local updates. arXiv preprint arXiv:2306.05100.

[48] Bardou, A., Thiran, P., & Begin, T. (2023). Relaxing the additivity constraints in decentralized no-regret high-dimensional bayesian optimization. arXiv preprint arXiv:2305.19838.

[49] Chen, J., Ma, Z., Guo, H., Ma, Y., Zhang, J., & Gong, Y. J. (2024). Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning. arXiv preprint arXiv:2402.02355.

[50] Ye, H., Xu, H., & Wang, H. Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Lightweight Optimizer and Small-scale Training Dataset. In The Twelfth International Conference on Learning Representations.

[51] Ustimenko, A., & Beznosikov, A. (2023). Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting. arXiv preprint arXiv:2310.06081.

[52] Wang, Z., Hu, B., Havens, A. J., Araujo, A., Zheng, Y., Chen, Y., & Jha, S. (2024). On the scalability and memory efficiency of semidefinite programs for Lipschitz constant estimation of neural networks. In The Twelfth International Conference on Learning Representations.

[53] Chavdarova, T., Yang, T., Pagliardini, M., & Jordan, M. I. (2022). A primal-dual approach to solving variational inequalities with general constraints. arXiv preprint arXiv:2210.15659.

[54] Sow, D., Lin, S., Wang, Z., & Liang, Y. (2023). Doubly Robust Instance-Reweighted Adversarial Training. arXiv preprint arXiv:2308.00311.

[55] Lin, Y., Ma, Y. A., Wang, Y. X., Redberg, R., & Bu, Z. (2023). [Tractable mcmc for private learning with pure and gaussian differential privacy](https://openreview.net/forum?id=pmweVpJ229). arXiv preprint arXiv:2310.14661. (Poster)

[56] Gu, X., Lyu, K., Arora, S., Zhang, J., & Huang, L. (2023). A Quadratic Synchronization Rule for Distributed Deep Learning. arXiv preprint arXiv:2310.14423.

[57] Guan, Z., Zhou, Y., & Liang, Y. (2024, February). On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback. In The Twelfth International Conference on Learning Representations.

[58] Murata, T., Niwa, K., Fukami, T., & Tyou, I. Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity. In The Twelfth International Conference on Learning Representations.

[59] Dexter, G., Ocejo, B., Keerthi, S., Gupta, A., Acharya, A., & Khanna, R. (2024). A precise characterization of sgd stability using loss surface geometry. arXiv preprint arXiv:2401.12332.

[60] Theodoropoulos, P., Liu, G. H., Chen, T., Saravanos, A. D., & Theodorou, E. A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER. In The Twelfth International Conference on Learning Representations.

[61] Tang, Z., Rybin, D., & Chang, T. H. (2023). Zeroth-order optimization meets human feedback: Provable learning via ranking oracles. arXiv preprint arXiv:2303.03751.

[62] Yuan, X., de Vazelhes, W., Gu, B., & Xiong, H. New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions. In The Twelfth International Conference on Learning Representations.

## Learning Theory

[1] Fu, S., He, F., Tian, X., & Tao, D. (2023). Convergence of bayesian bilevel optimization. In The Twelfth International Conference on Learning Representations. [OpenReview](https://openreview.net/forum?id=fLXpXa7iiz) (Spotlight)

[2] Glasgow, M. (2023). Sgd finds then tunes features in two-layer neural networks with near-optimal sample complexity: A case study in the xor problem. arXiv preprint arXiv:2309.15111. [OpenReview](https://openreview.net/forum?id=HgOJlxzB16) (Spotlight)

[3] Marion, P., Wu, Y. H., Sander, M. E., & Biau, G. (2023). Implicit regularization of deep residual networks towards neural ODEs. arXiv preprint arXiv:2309.01213. [OpenReview](https://openreview.net/forum?id=AbXGwqb5Ht) (Spotlight)

[4] Cabannes, V., Dohmatob, E., & Bietti, A. (2023). Scaling laws for associative memories. arXiv preprint arXiv:2310.02984. [OpenReview](https://openreview.net/forum?id=Tzh6xAJSll) (Spotlight)

[5] Liu, X., Zhang, H., Gu, B., & Chen, H. [General Stability Analysis for Zeroth-Order Optimization Algorithms](https://openreview.net/forum?id=AfhNyr73Ma). In The Twelfth International Conference on Learning Representations. (Poster)

[6] Robin, D. A., Scaman, K., & Lelarge, M. (2025). [Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks](https://openreview.net/forum?id=rBH7x87VfJ). arXiv preprint arXiv:2501.05930. (Poster)

[7] Lu, Z., Zhang, Q., Chen, X., Zhang, F., Woodruff, D., & Hazan, E. (2024). [Adaptive Regret for Bandits Made Possible: Two Queries Suffice](https://openreview.net/forum?id=AY9KyTGcnk). arXiv preprint arXiv:2401.09278. (Poster)

[8] Li, X., Deng, Y., Wu, J., Zhou, D., & Gu, Q. (2023). R[isk Bounds of Accelerated SGD for Overparameterized Linear Regression](https://openreview.net/forum?id=AcoXPIPh4A). arXiv preprint arXiv:2311.14222. (Poster)

[9] Ju, P., Ghosh, A., & Shroff, N. [Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning](https://openreview.net/forum?id=yoVq2BGQdP). In The Twelfth International Conference on Learning Representations. (Poster)

[10] Xiong, N., Liu, Z., Wang, Z., & Yang, Z. (2023). Sample-efficient multi-agent rl: An optimization perspective. arXiv preprint arXiv:2310.06243.

[11] Segert, S. (2023). Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem. arXiv preprint arXiv:2311.11093.