1. An Empirical Study of Bayesian Optimization: Acquisition Versus Partition. [pdf](https://jmlr.org/papers/volume22/18-220/18-220.pdf) [code](https://github.com/Eiii/opt_cmp)
2. From Low Probability to High Confidence in Stochastic Convex Optimization. [pdf](https://jmlr.org/papers/volume22/20-821/20-821.pdf)
3. Integrative Generalized Convex Clustering Optimization and Feature Selection for Mixed Multi-View Data. [pdf](https://jmlr.org/papers/volume22/19-1012/19-1012.pdf)
4. Optimization with Momentum: Dynamical, Control-Theoretic, and Symplectic Perspectives. [pdf](https://jmlr.org/papers/volume22/20-207/20-207.pdf)
5. A Lyapunov Analysis of Accelerated Methods in Optimization. [pdf](https://jmlr.org/papers/volume22/20-195/20-195.pdf)
6. Stochastic Proximal Methods for Non-Smooth Non-Convex Constrained Sparse Optimization. [pdf](https://jmlr.org/papers/volume22/20-287/20-287.pdf)
7. Sparse Convex Optimization via Adaptively Regularized Hard Thresholding. [pdf](https://jmlr.org/papers/volume22/20-661/20-661.pdf)
8. Bandit Convex Optimization in Non-stationary Environments. [pdf](https://jmlr.org/papers/volume22/20-763/20-763.pdf)
9. Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives. [pdf](https://jmlr.org/papers/volume22/19-1049/19-1049.pdf) [code](https://github.com/hazimehh/L0Learn)
10. Estimation and Optimization of Composite Outcomes. [pdf](https://jmlr.org/papers/volume22/20-429/20-429.pdf)
11. Replica Exchange for Non-Convex Optimization. [pdf](https://jmlr.org/papers/volume22/20-697/20-697.pdf)
12. Stochastic Online Optimization using Kalman Recursion. [pdf](https://jmlr.org/papers/volume22/20-618/20-618.pdf)
13. Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning. [pdf](https://jmlr.org/papers/volume22/21-0259/21-0259.pdf) [code](https://github.com/PhilippeSu/KV-CBO)
14. On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes. [pdf](https://jmlr.org/papers/volume22/20-1259/20-1259.pdf)
15. Fast Learning for Renewal Optimization in Online Task Scheduling. [pdf](https://jmlr.org/papers/volume22/20-813/20-813.pdf)
16. A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines. [pdf](https://jmlr.org/papers/volume22/21-0641/21-0641.pdf)