## Neural Networks
1. Pursuit of the Cluster Structure of Network Lasso: Recovery Condition and Non-convex Extension [pdf](https://jmlr.org/papers/volume25/21-1190/21-1190.pdf)

## Non-convex Optimization
1. Convergence for nonconvex ADMM, with applications to CT imaging [pdf](https://jmlr.org/papers/volume25/21-0831/21-0831.pdf) [code](https://github.com/rinafb/ADMM_CT)
2. Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo under local conditions for nonconvex optimization. [pdf](https://jmlr.org/papers/volume25/21-1423/21-1423.pdf)
3. An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization. [pdf](https://jmlr.org/papers/volume25/23-1159/23-1159.pdf)
4. Stochastic Regularized Majorization-Minimization with weakly convex and multi-convex surrogates. [pdf](https://jmlr.org/papers/volume25/23-0349/23-0349.pdf) [code](https://github.com/HanbaekLyu/SRMM)
5. Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery. [pdf](https://jmlr.org/papers/volume25/24-0029/24-0029.pdf)
6. Scaled Conjugate Gradient Method for Nonconvex Optimization in Deep Neural Networks. [pdf](https://jmlr.org/papers/volume25/22-0815/22-0815.pdf) [code](https://github.com/iiduka-researches/202210-izumi)

## Convex Optimization
1. Scaling the Convex Barrier with Sparse Dual Algorithms. [pdf](https://jmlr.org/papers/volume25/21-0076/21-0076.pdf) [code](https://github.com/oval-group/oval-bab)
2. Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization. [pdf](https://jmlr.org/papers/volume25/21-0748/21-0748.pdf)
3. Faster Rates of Differentially Private Stochastic Convex Optimization. [pdf](https://jmlr.org/papers/volume25/22-0079/22-0079.pdf)
4. Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization. [pdf](https://jmlr.org/papers/volume25/23-1072/23-1072.pdf)
5. The Nystr√∂m method for convex loss functions. [pdf](https://jmlr.org/papers/volume25/23-0768/23-0768.pdf)

## Semi-definite programming
1. Efficient Convex Algorithms for Universal Kernel Learning. [pdf](https://jmlr.org/papers/volume25/23-0528/23-0528.pdf) [code](https://github.com/CyberneticSCL/TKL-version-0.9)

## Convergence
1. A projected semismooth Newton method for a class of nonconvex composite programs with strong prox-regularity. [pdf](https://jmlr.org/papers/volume25/23-0371/23-0371.pdf)
2.  High Probability Convergence Bounds for Non-convex Stochastic Gradient Descent with Sub-Weibull Noise. [pdf](https://jmlr.org/papers/volume25/23-0466/23-0466.pdf) [code](https://github.com/liammadden/sgd)